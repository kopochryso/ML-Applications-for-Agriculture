import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import matplotlib.pyplot as plt

# 1. Load data
field_data = pd.read_csv("field_data.csv")

# 2. Basic cleaning
# Convert date if needed
field_data["date"] = pd.to_datetime(field_data["date"], dayfirst=True, errors="coerce")

# Drop columns that are IDs or duplicates for now
X = field_data.drop(columns=["root_yield", "date", "site", "trial", "plot"])
y = field_data["root_yield"]

# Handle categorical variables (e.g., irrigation: yes/no)
X = pd.get_dummies(X, drop_first=True)

# 3. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Train baseline RandomForest
rf = RandomForestRegressor(n_estimators=300, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print("R²:", r2_score(y_test, y_pred))
print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))

# 5. Feature importance
importances = pd.Series(rf.feature_importances_, index=X.columns)
importances.sort_values(ascending=False).head(15).plot(kind="barh", figsize=(8,6))
plt.title("Top 15 Features for Root Yield Prediction")
plt.show()

# 6. Try XGBoost for comparison
xg_reg = xgb.XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xg_reg.fit(X_train, y_train)
y_pred_xgb = xg_reg.predict(X_test)

print("XGBoost R²:", r2_score(y_test, y_pred_xgb))
print("XGBoost RMSE:", mean_squared_error(y_test, y_pred_xgb, squared=False))
